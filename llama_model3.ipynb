{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers bitsandbytes accelerate safetensors\n"
      ],
      "metadata": {
        "id": "mOwj7ieYRc5C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step1: Importing the important libraries"
      ],
      "metadata": {
        "id": "ioGJ_Ia6VKVS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D6guSwMWRQpJ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "import re\n",
        "import logging\n",
        "import time\n",
        "from pypdf import PdfReader\n",
        "from pdf2image import convert_from_bytes\n",
        "import pytesseract\n",
        "from PIL import Image\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "# ===========================\n",
        "# Step 1:Setting up\n",
        "\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(\n",
        "    filename='invoice_extraction.log',\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s:%(levelname)s:%(message)s'\n",
        ")\n",
        "\n",
        "# Set up Llama model path and tokenizer/model\n",
        "MODEL_PATH = \"/kaggle/input/llama-3/transformers/8b-chat-hf/1\"  # Adjust to a smaller model if possible\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "\n",
        "# Load tokenizer and model without explicitly moving the model to the device\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_PATH,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        "    offload_buffers=True\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ==============================\n",
        "# 2. Functions\n",
        "\n",
        "\n",
        "def get_pdf_text(pdf_doc):\n",
        "    \"\"\"\n",
        "    Extracts text from a PDF file, handling regular and scanned PDFs.\n",
        "\n",
        "    Parameters:\n",
        "        pdf_doc (UploadedFile): The uploaded PDF file.\n",
        "\n",
        "    Returns:\n",
        "        str: The extracted text from the PDF.\n",
        "    \"\"\"\n",
        "    text = \"\"\n",
        "    try:\n",
        "        pdf_reader = PdfReader(pdf_doc)\n",
        "        num_pages = len(pdf_reader.pages)\n",
        "        for page_number, page in enumerate(pdf_reader.pages, start=1):\n",
        "            extracted_text = page.extract_text()\n",
        "            if extracted_text and len(extracted_text.strip()) > 50:  # Threshold for text extraction\n",
        "                text += extracted_text + \"\\n\"\n",
        "                logging.info(f\"Text extracted from page {page_number} using PdfReader.\")\n",
        "            else:\n",
        "                # If text extraction is insufficient, use OCR\n",
        "                logging.info(f\"Insufficient text on page {page_number}. Applying OCR.\")\n",
        "                pdf_doc.seek(0)  # Reset file pointer to read bytes\n",
        "                pdf_bytes = pdf_doc.read()\n",
        "                images = convert_from_bytes(pdf_bytes, first_page=page_number, last_page=page_number)\n",
        "                for image in images:\n",
        "                    ocr_text = pytesseract.image_to_string(image, config='--psm 6')  # Assume a single uniform block of text\n",
        "                    if ocr_text and len(ocr_text.strip()) > 10:  # Threshold for OCR text\n",
        "                        text += ocr_text + \"\\n\"\n",
        "                        logging.info(f\"OCR text extracted from page {page_number}.\")\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error extracting text from PDF: {e}\")\n",
        "        print(f\"Error extracting text from PDF: {e}\")\n",
        "    return text\n",
        "\n",
        "# Calling the Lama model\n",
        "def call_llama_model(pages_data, max_input_length=1000, timeout=300):\n",
        "    \"\"\"\n",
        "    Calls the Llama 3 model to extract invoice data in JSON format.\n",
        "\n",
        "    Parameters:\n",
        "        pages_data (str): The extracted text from the PDF.\n",
        "        max_input_length (int): Maximum number of tokens to send to the model.\n",
        "        timeout (int): Maximum time (in seconds) to wait for model inference.\n",
        "\n",
        "    Returns:\n",
        "        str or None: The raw extracted data from the model if successful; otherwise, None.\n",
        "    \"\"\"\n",
        "    prompt_template = '''Extract the following fields from the invoice data:\n",
        "- Invoice No.\n",
        "- Date\n",
        "- Amount\n",
        "- Total\n",
        "- Email\n",
        "- Place of Origin\n",
        "- Taxable Value\n",
        "- SGST Amount\n",
        "- CGST Amount\n",
        "- IGST Amount\n",
        "- SGST Rate\n",
        "- CGST Rate\n",
        "- IGST Rate\n",
        "- Tax Amount\n",
        "- Tax Rate\n",
        "- Final Amount\n",
        "- Invoice Date\n",
        "- Place of Supply\n",
        "- GSTIN Supplier\n",
        "\n",
        "Provide the output strictly in valid JSON format with no additional text, explanations, or comments.\n",
        "Ensure all keys are correctly spelled and correspond to the field names above.\n",
        "Do not include any trailing commas or syntax errors.\n",
        "\n",
        "Here is the invoice data:\n",
        "{pages}\n",
        "'''\n",
        "    # Limit the input text\n",
        "    pages_data = pages_data[:max_input_length]\n",
        "    prompt = prompt_template.format(pages=pages_data)\n",
        "\n",
        "    try:\n",
        "        print(\"Tokenizing input...\")\n",
        "        inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=max_input_length).to(model.device)\n",
        "        print(f\"Input shape: {inputs.input_ids.shape}\")\n",
        "\n",
        "        print(\"Starting model inference...\")\n",
        "        start_time = time.time()\n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=500,\n",
        "                temperature=0.3,\n",
        "                do_sample=True,\n",
        "                num_return_sequences=1,\n",
        "                eos_token_id=tokenizer.eos_token_id,\n",
        "                pad_token_id=tokenizer.eos_token_id,\n",
        "            )\n",
        "        inference_time = time.time() - start_time\n",
        "        print(f\"Model inference completed in {inference_time:.2f} seconds\")\n",
        "\n",
        "        llm_extracted_data = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        print(f\"Extracted data length: {len(llm_extracted_data)}\")\n",
        "        logging.info(f\"Raw model response for data extraction: {llm_extracted_data}\")\n",
        "        return llm_extracted_data\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Exception during model call: {e}\")\n",
        "        print(f\"An error occurred during model call: {e}\")\n",
        "        return None\n",
        "\n",
        "def validate_data(field, value):\n",
        "    \"\"\"\n",
        "    Validates the extracted data fields using regex patterns and assigns confidence levels.\n",
        "\n",
        "    Parameters:\n",
        "        field (str): The name of the field.\n",
        "        value (str): The extracted value of the field.\n",
        "\n",
        "    Returns:\n",
        "        tuple: (is_valid (bool), confidence (str))\n",
        "    \"\"\"\n",
        "    patterns  = {\n",
        "    'Invoice No.': r'^[A-Za-z0-9\\-]+$',                      # Alphanumeric characters and dashes\n",
        "    'Date': r'^\\d{2}/\\d{2}/\\d{4}$',                          # Format: DD/MM/YYYY\n",
        "    'Amount': r'^\\d+(\\.\\d{1,2})?$',                         # Decimal value (e.g., 100, 100.50)\n",
        "    'Total': r'^\\d+(\\.\\d{1,2})?$',                          # Decimal value (e.g., 100, 100.50)\n",
        "    'Email': r'^[\\w\\.-]+@[\\w\\.-]+\\.\\w+$',                   # Valid email format\n",
        "    'Place of Origin': r'^[A-Za-z\\s\\-]+$',                   # Alphabetic and spaces\n",
        "    'Taxable Value': r'^\\d+(\\.\\d{1,2})?$',                  # Decimal value (e.g., 100, 100.50)\n",
        "    'SGST Amount': r'^\\d+(\\.\\d{1,2})?$',                     # Decimal value (e.g., 100, 100.50)\n",
        "    'CGST Amount': r'^\\d+(\\.\\d{1,2})?$',                     # Decimal value (e.g., 100, 100.50)\n",
        "    'IGST Amount': r'^\\d+(\\.\\d{1,2})?$',                     # Decimal value (e.g., 100, 100.50)\n",
        "    'SGST Rate': r'^\\d+(\\.\\d{1,2})?$',                       # Percentage value\n",
        "    'CGST Rate': r'^\\d+(\\.\\d{1,2})?$',                       # Percentage value\n",
        "    'IGST Rate': r'^\\d+(\\.\\d{1,2})?$',                       # Percentage value\n",
        "    'Tax Amount': r'^\\d+(\\.\\d{1,2})?$',                      # Decimal value (e.g., 100, 100.50)\n",
        "    'Tax Rate': r'^\\d+(\\.\\d{1,2})?$',                        # Percentage value\n",
        "    'Final Amount': r'^\\d+(\\.\\d{1,2})?$',                   # Decimal value (e.g., 100, 100.50)\n",
        "    'Invoice Date': r'^\\d{2}/\\d{2}/\\d{4}$',                  # Format: DD/MM/YYYY\n",
        "    'Place of Supply': r'^[A-Za-z\\s\\-]+$',                   # Alphabetic and spaces\n",
        "    'GSTIN Supplier': r'^\\d{2}[A-Z]{5}\\d{4}[A-Z]{1}[A-Z\\d]{1}[Z]{1}[A-Z\\d]{1}$',  # GSTIN format\n",
        "}\n",
        "\n",
        "\n",
        "    if field in patterns:\n",
        "        if re.match(patterns[field], str(value).strip()):\n",
        "            return True, \"High Confidence\"\n",
        "        else:\n",
        "            return False, \"Low Confidence\"\n",
        "    else:\n",
        "        # For fields without specific patterns, basic non-empty check\n",
        "        if str(value).strip():\n",
        "            return True, \"Medium Confidence\"\n",
        "        else:\n",
        "            return False, \"Low Confidence\"\n",
        "\n",
        "def extract_json(raw_text):\n",
        "    \"\"\"\n",
        "    Extracts the first JSON object found in the raw text.\n",
        "\n",
        "    Parameters:\n",
        "        raw_text (str): The raw text containing JSON.\n",
        "\n",
        "    Returns:\n",
        "        str or None: The extracted JSON string if found; otherwise, None.\n",
        "    \"\"\"\n",
        "    pattern = r'\\{.*\\}'  # Matches the first occurrence of {...}\n",
        "    match = re.search(pattern, raw_text, re.DOTALL)\n",
        "    if match:\n",
        "        return match.group(0)\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "# ===========================================\n",
        "# 3. Core Function to Process PDF Files\n",
        "# ===========================================\n",
        "\n",
        "def create_docs(pdf_file_paths):\n",
        "    \"\"\"\n",
        "    Processes multiple PDF files to extract invoice data and compile it into a DataFrame.\n",
        "\n",
        "    Parameters:\n",
        "        pdf_file_paths (list): List of paths to PDF files.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: DataFrame containing all extracted invoice data.\n",
        "    \"\"\"\n",
        "    # Initialize DataFrame with additional columns for confidence and trust\n",
        "    df = pd.DataFrame(columns=[\n",
        "        'Invoice No.', 'Date', 'Amount', 'Total',\n",
        "        'Email', 'Place of Origin', 'Taxable Value', 'SGST Amount',\n",
        "        'CGST Amount', 'IGST Amount', 'SGST Rate', 'CGST Rate',\n",
        "        'IGST Rate', 'Tax Amount', 'Tax Rate', 'Final Amount',\n",
        "        'Invoice Date', 'Place of Supply',\n",
        "        'GSTIN Supplier',\n",
        "        'Confidence', 'Trust'\n",
        "    ])\n",
        "\n",
        "    # Initialize a list to hold rows\n",
        "    rows = []\n",
        "\n",
        "    # Metrics tracking\n",
        "    metrics = {\n",
        "        'total_files': 0,\n",
        "        'successful_extractions': 0,\n",
        "        'field_accuracy': {field: {'correct': 0, 'total': 0} for field in df.columns if field not in ['Confidence', 'Trust']}\n",
        "    }\n",
        "\n",
        "    for file_path in pdf_file_paths:\n",
        "        metrics['total_files'] += 1\n",
        "        file_name = os.path.basename(file_path)\n",
        "        print(f\"### Processing {file_name}...\")\n",
        "\n",
        "        # Read and extract text from PDF\n",
        "        pdf_doc = open(file_path, 'rb')\n",
        "        pdf_text = get_pdf_text(pdf_doc)\n",
        "        pdf_doc.close()\n",
        "\n",
        "        # Call the Llama model for data extraction\n",
        "        llm_extracted_data = call_llama_model(pdf_text)\n",
        "\n",
        "        if llm_extracted_data:\n",
        "            # Extract JSON from model output\n",
        "            json_data = extract_json(llm_extracted_data)\n",
        "            if json_data:\n",
        "                try:\n",
        "                    extracted_fields = json.loads(json_data)\n",
        "                    row = {field: extracted_fields.get(field, \"\") for field in df.columns[:-2]}  # All fields except Confidence and Trust\n",
        "                    # Validate each field\n",
        "                    for field, value in row.items():\n",
        "                        is_valid, confidence = validate_data(field, value)\n",
        "                        metrics['field_accuracy'][field]['total'] += 1\n",
        "                        if is_valid:\n",
        "                            metrics['field_accuracy'][field]['correct'] += 1\n",
        "\n",
        "                    row['Confidence'] = confidence\n",
        "                    row['Trust'] = \"High\" if all([validate_data(f, row[f])[0] for f in row]) else \"Low\"\n",
        "\n",
        "                    # Add the row to the list\n",
        "                    rows.append(row)\n",
        "                    metrics['successful_extractions'] += 1\n",
        "                except json.JSONDecodeError as e:\n",
        "                    logging.error(f\"JSON decode error for {file_name}: {e}\")\n",
        "                    print(f\"JSON decode error for {file_name}: {e}\")\n",
        "            else:\n",
        "                logging.warning(f\"No valid JSON found in model output for {file_name}.\")\n",
        "                print(f\"No valid JSON found in model output for {file_name}.\")\n",
        "        else:\n",
        "            logging.error(f\"Failed to extract data for {file_name}.\")\n",
        "\n",
        "    # Convert the list of rows to a DataFrame\n",
        "    df = pd.DataFrame(rows)\n",
        "\n",
        "    # Log metrics\n",
        "    logging.info(f\"Processed {metrics['total_files']} files with {metrics['successful_extractions']} successful extractions.\")\n",
        "    for field, accuracy in metrics['field_accuracy'].items():\n",
        "        logging.info(f\"Field: {field}, Accuracy: {accuracy['correct']}/{accuracy['total']}\")\n",
        "\n",
        "    return df\n",
        "\n",
        "# ==============================\n",
        "# 4. Running the Invoice Extraction\n",
        "# ==============================\n",
        "\n",
        "# Example: Process files from a given directory\n",
        "pdf_directory = \"/kaggle/input/test-data\"\n",
        "pdf_file_paths = [os.path.join(pdf_directory, file) for file in os.listdir(pdf_directory) if file.endswith('.pdf')]\n",
        "print(f\"Found {len(pdf_file_paths)} PDF files. Processing...\")\n",
        "\n",
        "# Process and extract data into DataFrame\n",
        "extracted_data_df = create_docs(pdf_file_paths)\n",
        "\n",
        "# Optionally, save the extracted DataFrame to CSV\n",
        "output_csv_path = \"extracted_invoice_data.csv\"\n",
        "extracted_data_df.to_csv(output_csv_path, index=False)\n",
        "print(f\"Extraction completed. Data saved to {output_csv_path}.\")\n"
      ],
      "metadata": {
        "id": "Jmj5x4J8RUy7",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gfLuL_v1Vl_C"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}